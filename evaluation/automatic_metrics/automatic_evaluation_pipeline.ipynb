{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89ecf773-b332-45f4-8508-3391fab33dc1",
   "metadata": {},
   "source": [
    "Covert student excel and save all dialogues in jsonï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b842f297-11eb-4f51-a615-f251f97f1ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "\n",
    "def find_matching_column(columns, keyword):\n",
    "    for col in columns:\n",
    "        if keyword.lower() in col.lower():\n",
    "            return col\n",
    "    return None\n",
    "\n",
    "def parse_dialogue(text):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return []\n",
    "\n",
    "    turns = []\n",
    "    # ä½¿ç”¨æ­£åˆ™åˆ‡å‰²æ¯ä¸ªå‘è¨€æ®µï¼Œé¿å…æˆªæ–­å†…å®¹\n",
    "    segments = re.split(r'\\n*(?=(C|P):)', text.strip())\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(segments) - 1:\n",
    "        if segments[i] in ['C', 'P']:\n",
    "            speaker_code = segments[i]\n",
    "            content = segments[i + 1].lstrip(':').strip()\n",
    "            role = \"assistant\" if speaker_code == \"C\" else \"user\"\n",
    "            speaker = \"doctor\" if speaker_code == \"C\" else \"patient\"\n",
    "            turns.append({\n",
    "                \"role\": role,\n",
    "                \"speaker\": speaker,\n",
    "                \"content\": f\"** {content}\"\n",
    "            })\n",
    "            i += 2\n",
    "        else:\n",
    "            i += 1\n",
    "    return turns\n",
    "\n",
    "# Batch process files in a folder\n",
    "input_folder = \"test\"\n",
    "output_file = \"all_dialogues.json\"\n",
    "all_dialogues = []\n",
    "seen_dialogues = set()  # To track unique dialogues\n",
    "\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith(\".csv\") or filename.endswith(\".tsv\"):\n",
    "        filepath = os.path.join(input_folder, filename)\n",
    "        sep = \"\\t\" if filename.endswith(\".tsv\") else \",\"\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(\n",
    "                filepath,\n",
    "                sep=sep,\n",
    "                encoding='utf-8',\n",
    "                quotechar='\"',\n",
    "                doublequote=True,\n",
    "                escapechar=\"\\\\\",\n",
    "                on_bad_lines='skip',\n",
    "                engine=\"python\"\n",
    "            ).fillna(\"\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Failed to load {filename}: {e}\")\n",
    "            continue\n",
    "\n",
    "        df.columns = [col.strip() for col in df.columns]\n",
    "        col_basic = find_matching_column(df.columns, \"basic conversation\")\n",
    "        col_physical = find_matching_column(df.columns, \"physical function\")\n",
    "        col_emotional = find_matching_column(df.columns, \"emotional feedback\")\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            basic = parse_dialogue(row.get(col_basic, \"\"))\n",
    "            physical = parse_dialogue(row.get(col_physical, \"\"))\n",
    "            emotional = parse_dialogue(row.get(col_emotional, \"\"))\n",
    "            full = basic + physical + emotional\n",
    "\n",
    "            if full:\n",
    "                dialogue_str = json.dumps(full, ensure_ascii=False, sort_keys=True)\n",
    "                if dialogue_str not in seen_dialogues:\n",
    "                    all_dialogues.append(full)\n",
    "                    seen_dialogues.add(dialogue_str)\n",
    "\n",
    "# Save all unique dialogues\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_dialogues, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"âœ… Processed {len(all_dialogues)} unique dialogues and saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff6238d-29d0-420e-83c2-96279474f021",
   "metadata": {},
   "source": [
    "statistic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8574b14-cdda-44e0-bc1c-e225e26614ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š ICF Category D420\n",
      "Total nr. of conversations:   21\n",
      "Total nr. of turns:           379\n",
      "Average turns/Conversation:   18.05\n",
      "Average turn length (words):  16.94\n",
      "Avg words/Dialogue:           305.81\n",
      "\n",
      "ğŸ“Š ICF Category D445\n",
      "Total nr. of conversations:   18\n",
      "Total nr. of turns:           335\n",
      "Average turns/Conversation:   18.61\n",
      "Average turn length (words):  15.44\n",
      "Avg words/Dialogue:           287.44\n",
      "\n",
      "ğŸ“Š ICF Category D465\n",
      "Total nr. of conversations:   21\n",
      "Total nr. of turns:           406\n",
      "Average turns/Conversation:   19.33\n",
      "Average turn length (words):  14.38\n",
      "Avg words/Dialogue:           277.95\n",
      "\n",
      "ğŸ“Š ICF Category D470\n",
      "Total nr. of conversations:   25\n",
      "Total nr. of turns:           487\n",
      "Average turns/Conversation:   19.48\n",
      "Average turn length (words):  13.8\n",
      "Avg words/Dialogue:           268.76\n",
      "\n",
      "ğŸ“Š All Categories (Overall)\n",
      "Total nr. of conversations:   85\n",
      "Total nr. of turns:           1607\n",
      "Average turns/Conversation:   18.91\n",
      "Average turn length (words):  15.03\n",
      "Avg words/Dialogue:           284.14\n",
      "\n",
      "--- SAMPLE (D420) turns=18 ---\n",
      "[1] doctor: Hey Cynthia, how have you been doing?\n",
      "[2] patient: Honestly, not so well.\n",
      "[3] doctor: Iâ€™m sorry to hear that, can you explain why that is the case?\n",
      "[4] patient: Iâ€™ve been struggling transferring from my wheelchair to my bed.\n",
      "[5] doctor: How annoying. You said â€œrecentlyâ€, can you tell me how many days you have been struggling with this issue.\n",
      "[6] patient: The exact amount of days Iâ€™m unsure of but it has been getting progressively harder.\n",
      "[7] doctor: OK. Iâ€™ll try my best to help you. So, can you elaborate a little further by telling me what exactly is limiting your ability to transfer fro\n",
      "[8] patient: I no longer have the strength for it.\n",
      "\n",
      "--- SAMPLE (D420) turns=24 ---\n",
      "[1] doctor: Good afternoon maâ€™am, my name is John. Whatâ€™s your name?\n",
      "[2] patient: Good afternoon John, my name is Sarah.\n",
      "[3] doctor: Since this is the first time we meet, is it okay if I ask a few personal questions?\n",
      "[4] patient: Yes of course!\n",
      "[5] doctor: OK. So, how old are and could you describe your living environment.\n",
      "[6] patient: I am 71 years old and for the past 40 years I have been living with my husband.\n",
      "[7] doctor: What did you do for work?\n",
      "[8] patient: I used to be a data analyst.\n",
      "\n",
      "--- SAMPLE (D420) turns=20 ---\n",
      "[1] doctor: Hey, my name is Elisabeth, I will be your physiotherapist for today.\n",
      "[2] patient: Hi Elisabeth, my name Susanne, nice to meet you.\n",
      "[3] doctor: Nice to meet you too. How old are you Elisabeth?\n",
      "[4] patient: I am 63 years old.\n",
      "[5] doctor: Alright Susanne, what can I help you with?\n",
      "[6] patient: I have had 2 total knee replacements and feel as if I never fully gained my strength back. Itâ€™s been difficult getting up from a seated posi\n",
      "[7] doctor: Thatâ€™s unfortunate. So, itâ€™s difficult but not impossible?\n",
      "[8] patient: Yeah, Iâ€™m able to stand up but it just costs me a lot of energy.\n",
      "\n",
      "[INFO] CSV exported to: /Users/cheryl/Desktop/Internship&Thesis/code/data/test data/test_corpus_overview_stats.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import csv\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# =========================\n",
    "# Config\n",
    "# =========================\n",
    "input_folder = \"test\"\n",
    "EXPORT_CSV = True\n",
    "CSV_PATH = \"test_corpus_overview_stats.csv\"\n",
    "DEBUG_SHOW_SAMPLES = 3  # æ‰“å°å‰ N æ®µè§£ææ ·æœ¬ï¼ˆè®¾ 0 å…³é—­ï¼‰\n",
    "\n",
    "# æ”¯æŒå¤šç§è¡¨å¤´å†™æ³•ï¼šbasic/physical/emotional ä»¥åŠ part one/two/three\n",
    "CANDIDATE_KEYS = {\n",
    "    \"basic\": [\n",
    "        \"basic conversation\", \"basic consultation\", \"basic\", \"intro\",\n",
    "        \"introduction\", \"opening\", \"greeting\",\n",
    "        \"part one\", \"part 1\", \"section one\", \"section 1\"\n",
    "    ],\n",
    "    \"physical\": [\n",
    "        \"physical function\", \"functional follow-up\", \"functional details\",\n",
    "        \"functional section\", \"function\", \"follow-up\", \"physical\",\n",
    "        \"part two\", \"part 2\", \"section two\", \"section 2\"\n",
    "    ],\n",
    "    \"emotional\": [\n",
    "        \"emotional feedback\", \"emotional\", \"affective\", \"emotion\",\n",
    "        \"emotional perspective\", \"psychological\",\n",
    "        \"part three\", \"part 3\", \"section three\", \"section 3\"\n",
    "    ],\n",
    "}\n",
    "\n",
    "# å¯èƒ½æ˜¯æ¨¡æ¿è¯´æ˜çš„å…³é”®è¯ï¼ˆå‡ºç°ä¸”æ— æ ‡ç­¾æ—¶ä¸¢å¼ƒè¯¥æ®µï¼‰\n",
    "INSTRUCTION_HINTS = [\n",
    "    \"rounds\", \"greetings\", \"small talk\", \"variations\", \"inherent order\",\n",
    "    \"logically follow\", \"function disability\", \"make it natural\",\n",
    "    \"severity level\", \"give more variations\"\n",
    "]\n",
    "\n",
    "# =========================\n",
    "# Column helpers\n",
    "# =========================\n",
    "def find_matching_column(columns, key_phrase):\n",
    "    norm_cols = {c: re.sub(r\"\\s+\", \" \", str(c)).strip().lower() for c in columns}\n",
    "    for col, nc in norm_cols.items():\n",
    "        if key_phrase.lower() in nc:\n",
    "            return col\n",
    "    return None\n",
    "\n",
    "def find_first_hit(columns, candidates):\n",
    "    for cand in candidates:\n",
    "        col = find_matching_column(columns, cand)\n",
    "        if col:\n",
    "            return col\n",
    "    return None\n",
    "\n",
    "# =========================\n",
    "# Dialogue parsing\n",
    "# =========================\n",
    "def normalize_spaces(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", str(s)).strip()\n",
    "\n",
    "# ç»Ÿä¸€çš„æ ‡ç­¾æ£€æµ‹ï¼ˆè¦æ±‚æ ‡ç­¾å‰ä¸ºè¡Œé¦–æˆ–éå­—æ¯è¾¹ç•Œï¼‰\n",
    "TAG_PATTERN = re.compile(\n",
    "    r'(^|[^A-Za-z])(?:'\n",
    "    r'(?P<C>'                       # Clinician\n",
    "        r'C\\s*\\d*\\s*[:ï¼š.\\-]'       # C:, C1., C2-\n",
    "        r'|Doctor\\s*[:ï¼š]'          # Doctor:\n",
    "        r'|Dr\\s*[:ï¼š]'              # Dr:\n",
    "        r'|Clinician\\s*[:ï¼š]'       # Clinician:\n",
    "        r'|åŒ»ç”Ÿ\\s*[:ï¼š]'            # ä¸­æ–‡ï¼šåŒ»ç”Ÿ:\n",
    "        r'|Q\\s*[:ï¼š]'               # Q: è§†ä¸º Clinician\n",
    "    r')'\n",
    "    r'|'\n",
    "    r'(?P<P>'                       # Patient\n",
    "        r'P\\s*\\d*\\s*[:ï¼š.\\-]'       # P:, P1., P2-\n",
    "        r'|Patient\\s*[:ï¼š]'         # Patient:\n",
    "        r'|æ‚£è€…\\s*[:ï¼š]'             # ä¸­æ–‡ï¼šæ‚£è€…:\n",
    "        r'|ç—…äºº\\s*[:ï¼š]'             # ä¸­æ–‡ï¼šç—…äºº:\n",
    "        r'|A\\s*[:ï¼š]'               # A: è§†ä¸º Patient\n",
    "    r'))',\n",
    "    flags=re.IGNORECASE | re.MULTILINE | re.DOTALL\n",
    ")\n",
    "\n",
    "def has_speaker_labels(text: str) -> bool:\n",
    "    return bool(TAG_PATTERN.search(text or \"\"))\n",
    "\n",
    "def looks_like_dialogue(text: str) -> bool:\n",
    "    \"\"\"åŒ…å«æ ‡ç­¾åˆ™è®¤ä¸ºåƒå¯¹è¯ï¼›è‹¥æ— æ ‡ç­¾ä¸”å‘½ä¸­è¯´æ˜è¯ï¼Œåˆ™ä¸¢å¼ƒã€‚\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return False\n",
    "    t = text.strip()\n",
    "    if not t:\n",
    "        return False\n",
    "    if has_speaker_labels(t):\n",
    "        return True\n",
    "    low = t.lower()\n",
    "    if any(h in low for h in INSTRUCTION_HINTS):\n",
    "        return False\n",
    "    return False  # æ— æ ‡ç­¾ä¸”ä¸åƒå¯¹è¯\n",
    "\n",
    "def extract_turns_by_speaker(text: str):\n",
    "    \"\"\"\n",
    "    ç›´æ¥åœ¨åŸæ–‡ä¸Šæ‰«æçœŸæ­£çš„æ ‡ç­¾ä½ç½®ï¼Œå¹¶æŒ‰åŒºé—´æå–å†…å®¹ã€‚\n",
    "    ä¸å…ˆæ›¿æ¢ï¼Œé¿å…æŠŠ Cynthia/course/case ç­‰è¯¯åˆ‡ã€‚\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "\n",
    "    # ä»…ç»Ÿä¸€æ¢è¡Œï¼Œä¸åšæ›¿æ¢\n",
    "    t = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "\n",
    "    turns = []\n",
    "    last_role = None\n",
    "    last_end = None\n",
    "\n",
    "    for m in TAG_PATTERN.finditer(t):\n",
    "        role = \"doctor\" if m.group(\"C\") else \"patient\"\n",
    "        label_end = m.end()  # ä»æ ‡ç­¾æœ«å°¾å¼€å§‹æ˜¯å…¶å†…å®¹\n",
    "\n",
    "        if last_role is None:\n",
    "            last_role = role\n",
    "            last_end = label_end\n",
    "        else:\n",
    "            content = t[last_end:m.start()].strip()\n",
    "            content = normalize_spaces(content)\n",
    "            if content:\n",
    "                turns.append({\"speaker\": last_role, \"content\": content})\n",
    "            last_role = role\n",
    "            last_end = label_end\n",
    "\n",
    "    if last_role is not None and last_end is not None:\n",
    "        tail = normalize_spaces(t[last_end:])\n",
    "        if tail:\n",
    "            turns.append({\"speaker\": last_role, \"content\": tail})\n",
    "\n",
    "    # å…œåº•ï¼ˆæå°‘ï¼‰ï¼šæœªæ£€æµ‹åˆ°æ ‡ç­¾ï¼Œåˆ™ä¸è¿”å›ä»»ä½• turnï¼ˆé¿å…æŠŠæ•´æ®µè¯´æ˜å½“ä½œå¯¹è¯ï¼‰\n",
    "    return turns\n",
    "\n",
    "# =========================\n",
    "# Stats\n",
    "# =========================\n",
    "def words_in(text: str) -> int:\n",
    "    return len(normalize_spaces(text).split()) if isinstance(text, str) else 0\n",
    "\n",
    "def compute_stats(dialogues):\n",
    "    n = len(dialogues)\n",
    "    if n == 0:\n",
    "        return {\n",
    "            \"nr_conversations\": 0,\n",
    "            \"total_turns\": 0,\n",
    "            \"avg_turns_per_conversation\": 0.0,\n",
    "            \"avg_turn_length\": 0.0,\n",
    "            \"avg_words_per_dialogue\": 0.0,\n",
    "        }\n",
    "    turn_counts, dialogue_word_totals, all_turn_word_counts = [], [], []\n",
    "    for dlg in dialogues:\n",
    "        turn_counts.append(len(dlg))\n",
    "        wsum = 0\n",
    "        for t in dlg:\n",
    "            wc = words_in(t.get(\"content\", \"\"))\n",
    "            all_turn_word_counts.append(wc)\n",
    "            wsum += wc\n",
    "        dialogue_word_totals.append(wsum)\n",
    "\n",
    "    total_turns = int(np.sum(turn_counts))\n",
    "    return {\n",
    "        \"nr_conversations\": n,\n",
    "        \"total_turns\": total_turns,\n",
    "        \"avg_turns_per_conversation\": round(float(np.mean(turn_counts)), 2),\n",
    "        \"avg_turn_length\": round(float(np.mean(all_turn_word_counts)), 2),\n",
    "        \"avg_words_per_dialogue\": round(float(np.mean(dialogue_word_totals)), 2),\n",
    "    }\n",
    "\n",
    "# =========================\n",
    "# Main: read â†’ parse â†’ stats\n",
    "# =========================\n",
    "all_dialogues = []\n",
    "seen_dialogues = set()\n",
    "category_dialogue_tracker = defaultdict(list)\n",
    "category_counter = Counter()\n",
    "\n",
    "for filename in os.listdir(input_folder):\n",
    "    if not (filename.endswith(\".csv\") or filename.endswith(\".tsv\")):\n",
    "        continue\n",
    "\n",
    "    filepath = os.path.join(input_folder, filename)\n",
    "    sep = \"\\t\" if filename.endswith(\".tsv\") else \",\"\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(\n",
    "            filepath, sep=sep, encoding=\"utf-8\",\n",
    "            quotechar='\"', doublequote=True, escapechar=\"\\\\\",\n",
    "            on_bad_lines=\"skip\", engine=\"python\"\n",
    "        ).fillna(\"\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to load {filename}: {e}\")\n",
    "        continue\n",
    "\n",
    "    df.columns = [str(col).strip() for col in df.columns]\n",
    "\n",
    "    # ä¼˜å…ˆåŒ¹é…ä¸‰æ®µåˆ—ï¼›è‹¥åŒ¹é…å¤±è´¥ï¼Œå°†åœ¨è¡Œçº§åšæ–‡æœ¬æ‹¼æ¥å…œåº•\n",
    "    col_basic    = find_first_hit(df.columns, CANDIDATE_KEYS[\"basic\"])\n",
    "    col_physical = find_first_hit(df.columns, CANDIDATE_KEYS[\"physical\"])\n",
    "    col_emotional= find_first_hit(df.columns, CANDIDATE_KEYS[\"emotional\"])\n",
    "\n",
    "    # ICF ç±»åˆ«æ¥è‡ªæ–‡ä»¶å\n",
    "    m = re.search(r\"(D420|D445|D465|D470)\", filename, flags=re.I)\n",
    "    icf_category = m.group(1).upper() if m else \"UNKNOWN\"\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        parts = []\n",
    "        # ä¼˜å…ˆä½¿ç”¨ä¸‰æ®µåˆ—\n",
    "        if col_basic:     parts.append(str(row.get(col_basic, \"\")))\n",
    "        if col_physical:  parts.append(str(row.get(col_physical, \"\")))\n",
    "        if col_emotional: parts.append(str(row.get(col_emotional, \"\")))\n",
    "\n",
    "        # è‹¥ä¸‰æ®µéƒ½ç¼ºï¼Œå…œåº•ï¼šæ‹¼æ¥æœ¬è¡Œæ‰€æœ‰çœ‹èµ·æ¥åƒå¯¹è¯çš„æ–‡æœ¬åˆ—\n",
    "        if not any([col_basic, col_physical, col_emotional]):\n",
    "            row_texts = []\n",
    "            for col in df.columns:\n",
    "                val = row.get(col, \"\")\n",
    "                if isinstance(val, str) and val.strip() and looks_like_dialogue(val):\n",
    "                    row_texts.append(val)\n",
    "            parts = row_texts\n",
    "\n",
    "        # åˆå¹¶ä¸ºå®Œæ•´å¯¹è¯æ–‡æœ¬\n",
    "        full_text = \"\\n\".join([p for p in parts if isinstance(p, str) and p.strip()])\n",
    "\n",
    "        # å¿…é¡»å«æœ‰æ ‡ç­¾ï¼Œå¦åˆ™è·³è¿‡ï¼ˆé¿å…æŠŠâ€œè¯´æ˜è¡Œâ€å½“å¯¹è¯ï¼‰\n",
    "        if not has_speaker_labels(full_text):\n",
    "            continue\n",
    "\n",
    "        turns = extract_turns_by_speaker(full_text)\n",
    "        if not turns:\n",
    "            continue\n",
    "\n",
    "        # å»é‡ï¼šæŒ‰ turn åºåˆ— JSON\n",
    "        key = json.dumps(turns, ensure_ascii=False, sort_keys=True)\n",
    "        if key in seen_dialogues:\n",
    "            continue\n",
    "        seen_dialogues.add(key)\n",
    "\n",
    "        all_dialogues.append(turns)\n",
    "        category_dialogue_tracker[icf_category].append(turns)\n",
    "\n",
    "# Per-category counts\n",
    "for cat, dialogues in category_dialogue_tracker.items():\n",
    "    category_counter[cat] = len(dialogues)\n",
    "\n",
    "# Compute stats\n",
    "per_category_stats = {cat: compute_stats(dialogues)\n",
    "                      for cat, dialogues in sorted(category_dialogue_tracker.items())}\n",
    "overall_stats = compute_stats(all_dialogues)\n",
    "\n",
    "# =========================\n",
    "# Print\n",
    "# =========================\n",
    "def print_block(title, stats):\n",
    "    print(f\"\\nğŸ“Š {title}\")\n",
    "    print(f\"Total nr. of conversations:   {stats['nr_conversations']}\")\n",
    "    print(f\"Total nr. of turns:           {stats['total_turns']}\")\n",
    "    print(f\"Average turns/Conversation:   {stats['avg_turns_per_conversation']}\")\n",
    "    print(f\"Average turn length (words):  {stats['avg_turn_length']}\")\n",
    "    print(f\"Avg words/Dialogue:           {stats['avg_words_per_dialogue']}\")\n",
    "\n",
    "for cat in sorted(per_category_stats.keys()):\n",
    "    print_block(f\"ICF Category {cat}\", per_category_stats[cat])\n",
    "print_block(\"All Categories (Overall)\", overall_stats)\n",
    "\n",
    "# =========================\n",
    "# Debug samples\n",
    "# =========================\n",
    "if DEBUG_SHOW_SAMPLES > 0:\n",
    "    shown = 0\n",
    "    for cat in sorted(category_dialogue_tracker.keys()):\n",
    "        for dlg in category_dialogue_tracker[cat][:DEBUG_SHOW_SAMPLES]:\n",
    "            print(f\"\\n--- SAMPLE ({cat}) turns={len(dlg)} ---\")\n",
    "            for i, t in enumerate(dlg[:8], 1):\n",
    "                print(f\"[{i}] {t['speaker']}: {t['content'][:140]}\")\n",
    "            shown += 1\n",
    "            if shown >= DEBUG_SHOW_SAMPLES:\n",
    "                break\n",
    "        if shown >= DEBUG_SHOW_SAMPLES:\n",
    "            break\n",
    "\n",
    "# =========================\n",
    "# CSV export (optional)\n",
    "# =========================\n",
    "if EXPORT_CSV:\n",
    "    fieldnames = [\n",
    "        \"icf_category\",\n",
    "        \"nr_conversations\",\n",
    "        \"total_turns\",\n",
    "        \"avg_turns_per_conversation\",\n",
    "        \"avg_turn_length\",\n",
    "        \"avg_words_per_dialogue\",\n",
    "    ]\n",
    "    with open(CSV_PATH, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for cat, stats in sorted(per_category_stats.items()):\n",
    "            writer.writerow({\"icf_category\": cat, **stats})\n",
    "        writer.writerow({\"icf_category\": \"OVERALL\", **overall_stats})\n",
    "    print(f\"\\n[INFO] CSV exported to: {os.path.abspath(CSV_PATH)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5cd62d-0cd6-44e7-89cb-ff7180666dbd",
   "metadata": {},
   "source": [
    "Exact all follow-up questions from turn 3(the second C) and save them in json:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ea7c84-c325-418e-ad38-59fd9985b169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Input and output file paths\n",
    "input_file = \"data/test data/all_dialogues.json\"\n",
    "output_file = \"data/test data/followup_references_from_second_c_flat.json\"\n",
    "\n",
    "# Load dialogues\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    dialogues = json.load(f)\n",
    "\n",
    "# Store results\n",
    "followup_data = []\n",
    "\n",
    "for i, dialogue in enumerate(dialogues):\n",
    "    dialogue_id = i + 1\n",
    "\n",
    "    # Get all doctor utterances\n",
    "    doctor_turns = [\n",
    "        turn[\"content\"].lstrip(\"* \").strip()\n",
    "        for turn in dialogue\n",
    "        if turn.get(\"speaker\", \"\").lower() == \"doctor\"\n",
    "    ]\n",
    "\n",
    "    # Extract from the 2nd doctor turn (index 1)\n",
    "    followups = doctor_turns[1:] if len(doctor_turns) >= 2 else []\n",
    "\n",
    "    # Add each follow-up question as a separate item\n",
    "    for j, q in enumerate(followups):\n",
    "        followup_data.append({\n",
    "            \"dialogue_id\": dialogue_id,\n",
    "            \"turn_index\": j + 2,  # +2 to reflect the actual doctor turn index (starting from 0)\n",
    "            \"followup_question\": q\n",
    "        })\n",
    "\n",
    "# Save as JSON\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(followup_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"âœ… Flattened follow-up questions saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6801c73f-b718-4ffa-841b-b333daf023fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Input and output file paths\n",
    "input_file = \"data/test data/all_dialogues.json\"\n",
    "output_file = \"data/test data/followup_references_from_second_c.json\"\n",
    "\n",
    "# Load dialogues\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    dialogues = json.load(f)\n",
    "\n",
    "# Store results\n",
    "followup_data = []\n",
    "\n",
    "for i, dialogue in enumerate(dialogues):\n",
    "    # Get all doctor utterances\n",
    "    doctor_turns = [\n",
    "        turn[\"content\"].lstrip(\"* \").strip()\n",
    "        for turn in dialogue\n",
    "        if turn.get(\"speaker\", \"\").lower() == \"doctor\"\n",
    "    ]\n",
    "    \n",
    "    # Extract from the 2nd doctor turn (index 1)\n",
    "    followups = doctor_turns[1:] if len(doctor_turns) >= 2 else []\n",
    "    \n",
    "    followup_data.append({\n",
    "        \"dialogue_id\": i + 1,\n",
    "        \"followup_questions\": followups\n",
    "    })\n",
    "\n",
    "# Save as JSON\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(followup_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"âœ… Extracted follow-up questions from the 2nd doctor turn onward. Saved to {output_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0386415f-4941-4047-a6b2-caf5051b6fc9",
   "metadata": {},
   "source": [
    "Generate 10 variants for each follow-up questionsï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8eaaba-8334-4655-9d07-3639d8afd654",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "\n",
    "# ========== Config ==========\n",
    "input_file = \"data/test data/all_dialogues.json\"\n",
    "output_file = \"data/test data/doctor_paraphrase_references_with_context.json\"\n",
    "\n",
    "# Local LLM API\n",
    "client = OpenAI(base_url=\"http://localhost:8000/v1\", api_key=\"cltl\")\n",
    "model_name = \"Meta-Llama-3.1-8B-Instruct.Q4_K_M.gguf\"  # Replace with your registered local model name\n",
    "\n",
    "# Load dialogues\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    dialogues = json.load(f)\n",
    "\n",
    "# Extract follow-up questions with context\n",
    "question_records = []\n",
    "for dialogue_id, dialogue in enumerate(dialogues, start=1):\n",
    "    context_turns = []\n",
    "    doctor_turn_count = 0\n",
    "    \n",
    "    for turn in dialogue:\n",
    "        speaker = turn.get(\"speaker\", \"\").lower()\n",
    "        content = turn.get(\"content\", \"\").lstrip(\"* \").strip()\n",
    "        \n",
    "        if speaker == \"doctor\":\n",
    "            doctor_turn_count += 1\n",
    "            if doctor_turn_count >= 2:\n",
    "                question_records.append({\n",
    "                    \"dialogue_id\": dialogue_id,\n",
    "                    \"question_index\": doctor_turn_count - 2,\n",
    "                    \"context\": \" \".join(context_turns).strip(),\n",
    "                    \"question\": content\n",
    "                })\n",
    "        context_turns.append(f\"{speaker.capitalize()}: {content}\")\n",
    "\n",
    "print(f\"âœ… Loaded {len(question_records)} questions with context\")\n",
    "\n",
    "# Build Prompt + Call Model\n",
    "def build_prompt(context, question):\n",
    "    return (\n",
    "        \"You are a helpful and professional assistant. \"\n",
    "        \"Below is a clinical conversation followed by a question asked by the doctor. \"\n",
    "        \"Rewrite this doctor's question into 10 different semantically equivalent variants, using the provided context.\\n\\n\"\n",
    "        f\"Conversation context:\\n{context}\\n\\n\"\n",
    "        f\"Doctor's question: \\\"{question}\\\"\\n\\n\"\n",
    "        \"Paraphrased versions:\\n\"\n",
    "    )\n",
    "\n",
    "def generate_variants(context, question):\n",
    "    prompt = build_prompt(context, question)\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.8,\n",
    "            top_p=0.95,\n",
    "            max_tokens=512,\n",
    "        )\n",
    "        content = response.choices[0].message.content.strip()\n",
    "        lines = content.split(\"\\n\")\n",
    "        variants = []\n",
    "\n",
    "        for line in lines:\n",
    "            match = re.match(r\"^\\d+[\\.\\)]?\\s*(.*)\", line.strip())\n",
    "            if match:\n",
    "                variants.append(match.group(1))\n",
    "            elif line.strip():\n",
    "                variants.append(line.strip())\n",
    "\n",
    "        return variants[:10]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed for question: {question}\\nError: {e}\")\n",
    "        return []\n",
    "\n",
    "# Generate paraphrases and save\n",
    "output = []\n",
    "for record in tqdm(question_records, desc=\"Generating paraphrases with context\"):\n",
    "    context = record[\"context\"]\n",
    "    question = record[\"question\"]\n",
    "    variants = generate_variants(context, question)\n",
    "    output.append({\n",
    "        \"dialogue_id\": record[\"dialogue_id\"],\n",
    "        \"question_index\": record[\"question_index\"],\n",
    "        \"original\": question,\n",
    "        \"context\": context,\n",
    "        \"paraphrases\": variants\n",
    "    })\n",
    "    time.sleep(1)  # To prevent API overload\n",
    "\n",
    "# Save JSON\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(output, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"âœ… Contextual paraphrase generation complete. Saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c21fb37-c477-4c5e-aae6-d649cdba9f62",
   "metadata": {},
   "source": [
    "evaluation attemptï¼š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681e4450-acca-49f0-a93d-7437a9d90b27",
   "metadata": {},
   "source": [
    "Calculate automatic metricsï¼š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cc3bf5-8f9f-48e2-aeb6-362affe133d9",
   "metadata": {},
   "source": [
    "base modelâ€”â€”0shotï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1d6be2ac-6820-44d3-aef0-1fa66ae7bcef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Aligned samples: 730, Skipped: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/730 [00:00<?, ?it/s]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 730/730 [01:19<00:00,  9.16it/s]\n",
      "  0%|                                                   | 0/729 [00:00<?, ?it/s]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 729/729 [04:14<00:00,  2.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Evaluation (Prediction vs Gold):\n",
      "BLEU (avg)      : 0.0334\n",
      "ROUGE-L (avg)   : 0.1706\n",
      "BERTScore F1    : 0.8572\n",
      "\n",
      "ğŸ“Š Evaluation (Prediction vs Variants, excluding first):\n",
      "BLEU (avg)      : 0.0656\n",
      "ROUGE-L (avg)   : 0.2469\n",
      "BERTScore F1    : 0.8662\n",
      "ğŸ“ Saved to 'base_zero_automatic_scores_separated.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from evaluate import load\n",
    "from tqdm import tqdm\n",
    "import sacrebleu\n",
    "\n",
    "# === æ–‡ä»¶è·¯å¾„ ===\n",
    "pred_file = \"prediction/basemodel_0shot_output.json\"\n",
    "para_file = \"doctor_paraphrase_references_with_context.json\"\n",
    "\n",
    "# === åŠ è½½æ•°æ® ===\n",
    "with open(pred_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    predictions = json.load(f)\n",
    "\n",
    "with open(para_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    para_list = json.load(f)\n",
    "\n",
    "# === æ•°æ®å¯¹é½ ===\n",
    "aligned_preds = []\n",
    "gold_refs = []\n",
    "variant_refs = []\n",
    "aligned_ids = []\n",
    "skipped = 0\n",
    "\n",
    "for i, item in enumerate(para_list):\n",
    "    if i >= len(predictions):\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    pred_item = predictions[i]\n",
    "    pred = pred_item.get(\"generated_followup\") or pred_item.get(\"prediction\")\n",
    "    if not isinstance(pred, str) or not pred.strip():\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    prediction = pred.strip()\n",
    "    original = item[\"original\"].strip()\n",
    "    paraphrases = item.get(\"paraphrases\", [])\n",
    "\n",
    "    # Exclude the first variant\n",
    "    variants = paraphrases[1:]\n",
    "\n",
    "    aligned_preds.append(prediction)\n",
    "    gold_refs.append(original)\n",
    "    variant_refs.append(variants)\n",
    "    aligned_ids.append(f'{item[\"dialogue_id\"]}_{i}')\n",
    "\n",
    "print(f\"\\nâœ… Aligned samples: {len(aligned_preds)}, Skipped: {skipped}\")\n",
    "\n",
    "# æ·»åŠ æ ‡è®°æ¥è¿½è¸ªæœ‰æ•ˆæ€§\n",
    "valid_gold = []\n",
    "valid_variants = []\n",
    "\n",
    "# === ä¿®æ”¹ compute_metrics å‡½æ•° ===\n",
    "def compute_metrics(preds, refs):\n",
    "    valid_preds, valid_refs, valid_idx = [], [], []\n",
    "    bleu_scores_all = [None] * len(preds)\n",
    "    rouge_scores_all = [None] * len(preds)\n",
    "    bert_scores_all = [None] * len(preds)\n",
    "\n",
    "    rouge = load(\"rouge\")\n",
    "    bertscore = load(\"bertscore\")\n",
    "\n",
    "    for idx, (p, r) in enumerate(zip(preds, refs)):\n",
    "        if r:  # refséç©º\n",
    "            valid_preds.append(p)\n",
    "            valid_refs.append(r)\n",
    "            valid_idx.append(idx)\n",
    "\n",
    "    # è®¡ç®—BLEU\n",
    "    bleu_scores = [sacrebleu.sentence_bleu(p, r).score / 100 for p, r in zip(valid_preds, valid_refs)]\n",
    "\n",
    "    # è®¡ç®—ROUGE\n",
    "    rouge_result = rouge.compute(\n",
    "        predictions=valid_preds,\n",
    "        references=valid_refs,\n",
    "        use_stemmer=True,\n",
    "        use_aggregator=False,\n",
    "        rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "    )\n",
    "\n",
    "    # è®¡ç®—BERTScore\n",
    "    F1_max = []\n",
    "    for pred, ref_group in tqdm(zip(valid_preds, valid_refs), total=len(valid_preds)):\n",
    "        best_F1 = max(bertscore.compute(\n",
    "            predictions=[pred] * len(ref_group),\n",
    "            references=ref_group,\n",
    "            lang=\"en\",\n",
    "            rescale_with_baseline=False,\n",
    "            use_fast_tokenizer=True\n",
    "        )[\"f1\"])\n",
    "        F1_max.append(best_F1)\n",
    "\n",
    "    # æŠŠè®¡ç®—å¥½çš„æœ‰æ•ˆåˆ†æ•°æ”¾å›åŸå§‹ä½ç½®ï¼Œæ— æ•ˆä½ç½®å¡«å……None\n",
    "    for i, idx in enumerate(valid_idx):\n",
    "        bleu_scores_all[idx] = bleu_scores[i]\n",
    "        rouge_scores_all[idx] = rouge_result[\"rougeL\"][i]\n",
    "        bert_scores_all[idx] = F1_max[i]\n",
    "\n",
    "    metrics = {\n",
    "        \"BLEU\": sum(bleu_scores) / len(bleu_scores),\n",
    "        \"ROUGE-L\": sum(rouge_result['rougeL']) / len(rouge_result['rougeL']),\n",
    "        \"BERTScore_F1\": sum(F1_max) / len(F1_max)\n",
    "    }\n",
    "\n",
    "    return metrics, bleu_scores_all, rouge_scores_all, bert_scores_all\n",
    "\n",
    "# === Metrics: Prediction vs Gold ===\n",
    "gold_metrics, gold_bleu, gold_rouge, gold_f1 = compute_metrics(aligned_preds, [[ref] for ref in gold_refs])\n",
    "\n",
    "# === Metrics: Prediction vs Variants ===\n",
    "variant_metrics, variant_bleu, variant_rouge, variant_f1 = compute_metrics(aligned_preds, variant_refs)\n",
    "\n",
    "# === æ‰“å°ç»“æœ ===\n",
    "print(\"\\nğŸ“Š Evaluation (Prediction vs Gold):\")\n",
    "print(f\"BLEU (avg)      : {gold_metrics['BLEU']:.4f}\")\n",
    "print(f\"ROUGE-L (avg)   : {gold_metrics['ROUGE-L']:.4f}\")\n",
    "print(f\"BERTScore F1    : {gold_metrics['BERTScore_F1']:.4f}\")\n",
    "\n",
    "print(\"\\nğŸ“Š Evaluation (Prediction vs Variants, excluding first):\")\n",
    "print(f\"BLEU (avg)      : {variant_metrics['BLEU']:.4f}\")\n",
    "print(f\"ROUGE-L (avg)   : {variant_metrics['ROUGE-L']:.4f}\")\n",
    "print(f\"BERTScore F1    : {variant_metrics['BERTScore_F1']:.4f}\")\n",
    "\n",
    "# === ä¿å­˜ CSVï¼ˆç¡®ä¿é•¿åº¦ä¸€è‡´ï¼‰ ===\n",
    "df = pd.DataFrame({\n",
    "    \"sample_id\": aligned_ids,\n",
    "    \"prediction\": aligned_preds,\n",
    "    \"gold_reference\": gold_refs,\n",
    "    \"variant_references\": [\" â€¢ \" + \"\\n â€¢ \".join(v) for v in variant_refs],\n",
    "    \"BLEU_gold\": gold_bleu,\n",
    "    \"ROUGE-L_gold\": gold_rouge,\n",
    "    \"BERTScore_F1_gold\": gold_f1,\n",
    "    \"BLEU_variants\": variant_bleu,\n",
    "    \"ROUGE-L_variants\": variant_rouge,\n",
    "    \"BERTScore_F1_variants\": variant_f1,\n",
    "})\n",
    "\n",
    "# ç©ºç¼ºä½ç½®å¯èƒ½ä¸ºNoneï¼Œå¯ä»¥å¡«å……ä¸ºNaNæˆ–å…¶ä»–å€¼\n",
    "df.to_csv(\"prediction/base_zero_automatic_scores_separated.csv\", index=False)\n",
    "print(\"ğŸ“ Saved to 'base_zero_automatic_scores_separated.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b7d2b7-09ba-4bd5-910e-c6fd8a84b414",
   "metadata": {},
   "source": [
    "base modelâ€”â€”fewshotï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4523bee6-ede8-41d3-88df-1fd7b1066a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Aligned samples: 730, Skipped: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/730 [00:00<?, ?it/s]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 730/730 [01:21<00:00,  9.01it/s]\n",
      "  0%|                                                   | 0/729 [00:00<?, ?it/s]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 729/729 [04:24<00:00,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Evaluation (Prediction vs Gold):\n",
      "BLEU (avg)      : 0.0404\n",
      "ROUGE-L (avg)   : 0.1916\n",
      "BERTScore F1    : 0.8578\n",
      "\n",
      "ğŸ“Š Evaluation (Prediction vs Variants, excluding first):\n",
      "BLEU (avg)      : 0.0708\n",
      "ROUGE-L (avg)   : 0.2578\n",
      "BERTScore F1    : 0.8651\n",
      "ğŸ“ Saved to 'base_few_automatic_scores_separated.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from evaluate import load\n",
    "from tqdm import tqdm\n",
    "import sacrebleu\n",
    "\n",
    "# === æ–‡ä»¶è·¯å¾„ ===\n",
    "pred_file = \"prediction/basemodel_fewshot_output.json\"\n",
    "para_file = \"doctor_paraphrase_references_with_context.json\"\n",
    "\n",
    "# === åŠ è½½æ•°æ® ===\n",
    "with open(pred_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    predictions = json.load(f)\n",
    "\n",
    "with open(para_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    para_list = json.load(f)\n",
    "\n",
    "# === æ•°æ®å¯¹é½ ===\n",
    "aligned_preds = []\n",
    "gold_refs = []\n",
    "variant_refs = []\n",
    "aligned_ids = []\n",
    "skipped = 0\n",
    "\n",
    "for i, item in enumerate(para_list):\n",
    "    if i >= len(predictions):\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    pred_item = predictions[i]\n",
    "    pred = pred_item.get(\"generated_followup\") or pred_item.get(\"prediction\")\n",
    "    if not isinstance(pred, str) or not pred.strip():\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    prediction = pred.strip()\n",
    "    original = item[\"original\"].strip()\n",
    "    paraphrases = item.get(\"paraphrases\", [])\n",
    "\n",
    "    # Exclude the first variant\n",
    "    variants = paraphrases[1:]\n",
    "\n",
    "    aligned_preds.append(prediction)\n",
    "    gold_refs.append(original)\n",
    "    variant_refs.append(variants)\n",
    "    aligned_ids.append(f'{item[\"dialogue_id\"]}_{i}')\n",
    "\n",
    "print(f\"\\nâœ… Aligned samples: {len(aligned_preds)}, Skipped: {skipped}\")\n",
    "\n",
    "# æ·»åŠ æ ‡è®°æ¥è¿½è¸ªæœ‰æ•ˆæ€§\n",
    "valid_gold = []\n",
    "valid_variants = []\n",
    "\n",
    "# === ä¿®æ”¹ compute_metrics å‡½æ•° ===\n",
    "def compute_metrics(preds, refs):\n",
    "    valid_preds, valid_refs, valid_idx = [], [], []\n",
    "    bleu_scores_all = [None] * len(preds)\n",
    "    rouge_scores_all = [None] * len(preds)\n",
    "    bert_scores_all = [None] * len(preds)\n",
    "\n",
    "    rouge = load(\"rouge\")\n",
    "    bertscore = load(\"bertscore\")\n",
    "\n",
    "    for idx, (p, r) in enumerate(zip(preds, refs)):\n",
    "        if r:  # refséç©º\n",
    "            valid_preds.append(p)\n",
    "            valid_refs.append(r)\n",
    "            valid_idx.append(idx)\n",
    "\n",
    "    # è®¡ç®—BLEU\n",
    "    bleu_scores = [sacrebleu.sentence_bleu(p, r).score / 100 for p, r in zip(valid_preds, valid_refs)]\n",
    "\n",
    "    # è®¡ç®—ROUGE\n",
    "    rouge_result = rouge.compute(\n",
    "        predictions=valid_preds,\n",
    "        references=valid_refs,\n",
    "        use_stemmer=True,\n",
    "        use_aggregator=False,\n",
    "        rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "    )\n",
    "\n",
    "    # è®¡ç®—BERTScore\n",
    "    F1_max = []\n",
    "    for pred, ref_group in tqdm(zip(valid_preds, valid_refs), total=len(valid_preds)):\n",
    "        best_F1 = max(bertscore.compute(\n",
    "            predictions=[pred] * len(ref_group),\n",
    "            references=ref_group,\n",
    "            lang=\"en\",\n",
    "            rescale_with_baseline=False,\n",
    "            use_fast_tokenizer=True\n",
    "        )[\"f1\"])\n",
    "        F1_max.append(best_F1)\n",
    "\n",
    "    # æŠŠè®¡ç®—å¥½çš„æœ‰æ•ˆåˆ†æ•°æ”¾å›åŸå§‹ä½ç½®ï¼Œæ— æ•ˆä½ç½®å¡«å……None\n",
    "    for i, idx in enumerate(valid_idx):\n",
    "        bleu_scores_all[idx] = bleu_scores[i]\n",
    "        rouge_scores_all[idx] = rouge_result[\"rougeL\"][i]\n",
    "        bert_scores_all[idx] = F1_max[i]\n",
    "\n",
    "    metrics = {\n",
    "        \"BLEU\": sum(bleu_scores) / len(bleu_scores),\n",
    "        \"ROUGE-L\": sum(rouge_result['rougeL']) / len(rouge_result['rougeL']),\n",
    "        \"BERTScore_F1\": sum(F1_max) / len(F1_max)\n",
    "    }\n",
    "\n",
    "    return metrics, bleu_scores_all, rouge_scores_all, bert_scores_all\n",
    "\n",
    "# === Metrics: Prediction vs Gold ===\n",
    "gold_metrics, gold_bleu, gold_rouge, gold_f1 = compute_metrics(aligned_preds, [[ref] for ref in gold_refs])\n",
    "\n",
    "# === Metrics: Prediction vs Variants ===\n",
    "variant_metrics, variant_bleu, variant_rouge, variant_f1 = compute_metrics(aligned_preds, variant_refs)\n",
    "\n",
    "# === æ‰“å°ç»“æœ ===\n",
    "print(\"\\nğŸ“Š Evaluation (Prediction vs Gold):\")\n",
    "print(f\"BLEU (avg)      : {gold_metrics['BLEU']:.4f}\")\n",
    "print(f\"ROUGE-L (avg)   : {gold_metrics['ROUGE-L']:.4f}\")\n",
    "print(f\"BERTScore F1    : {gold_metrics['BERTScore_F1']:.4f}\")\n",
    "\n",
    "print(\"\\nğŸ“Š Evaluation (Prediction vs Variants, excluding first):\")\n",
    "print(f\"BLEU (avg)      : {variant_metrics['BLEU']:.4f}\")\n",
    "print(f\"ROUGE-L (avg)   : {variant_metrics['ROUGE-L']:.4f}\")\n",
    "print(f\"BERTScore F1    : {variant_metrics['BERTScore_F1']:.4f}\")\n",
    "\n",
    "# === ä¿å­˜ CSVï¼ˆç¡®ä¿é•¿åº¦ä¸€è‡´ï¼‰ ===\n",
    "df = pd.DataFrame({\n",
    "    \"sample_id\": aligned_ids,\n",
    "    \"prediction\": aligned_preds,\n",
    "    \"gold_reference\": gold_refs,\n",
    "    \"variant_references\": [\" â€¢ \" + \"\\n â€¢ \".join(v) for v in variant_refs],\n",
    "    \"BLEU_gold\": gold_bleu,\n",
    "    \"ROUGE-L_gold\": gold_rouge,\n",
    "    \"BERTScore_F1_gold\": gold_f1,\n",
    "    \"BLEU_variants\": variant_bleu,\n",
    "    \"ROUGE-L_variants\": variant_rouge,\n",
    "    \"BERTScore_F1_variants\": variant_f1,\n",
    "})\n",
    "\n",
    "# ç©ºç¼ºä½ç½®å¯èƒ½ä¸ºNoneï¼Œå¯ä»¥å¡«å……ä¸ºNaNæˆ–å…¶ä»–å€¼\n",
    "df.to_csv(\"prediction/base_few_automatic_scores_separated.csv\", index=False)\n",
    "print(\"ğŸ“ Saved to 'base_few_automatic_scores_separated.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8f77a1-1444-401a-a829-069c2fa48f4c",
   "metadata": {},
   "source": [
    "fine-tuned qwen3â€”â€”0shotï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "24d6bf20-c19f-46a3-ae03-e2ca48c4dcef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Aligned samples: 729, Skipped: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/729 [00:00<?, ?it/s]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 729/729 [01:05<00:00, 11.10it/s]\n",
      "  0%|                                                   | 0/728 [00:00<?, ?it/s]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 728/728 [02:48<00:00,  4.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Evaluation (Prediction vs Gold):\n",
      "BLEU (avg)      : 0.0553\n",
      "ROUGE-L (avg)   : 0.1986\n",
      "BERTScore F1    : 0.8846\n",
      "\n",
      "ğŸ“Š Evaluation (Prediction vs Variants, excluding first):\n",
      "BLEU (avg)      : 0.0966\n",
      "ROUGE-L (avg)   : 0.2690\n",
      "BERTScore F1    : 0.8848\n",
      "ğŸ“ Saved to 'fine_zero_automatic_scores_separated.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from evaluate import load\n",
    "from tqdm import tqdm\n",
    "import sacrebleu\n",
    "\n",
    "# === æ–‡ä»¶è·¯å¾„ ===\n",
    "pred_file = \"prediction/finetuned_0shot_output.json\"\n",
    "para_file = \"doctor_paraphrase_references_with_context.json\"\n",
    "\n",
    "# === åŠ è½½æ•°æ® ===\n",
    "with open(pred_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    predictions = json.load(f)\n",
    "\n",
    "with open(para_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    para_list = json.load(f)\n",
    "\n",
    "# === æ•°æ®å¯¹é½ ===\n",
    "aligned_preds = []\n",
    "gold_refs = []\n",
    "variant_refs = []\n",
    "aligned_ids = []\n",
    "skipped = 0\n",
    "\n",
    "for i, item in enumerate(para_list):\n",
    "    if i >= len(predictions):\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    pred_item = predictions[i]\n",
    "    pred = pred_item.get(\"generated_followup\") or pred_item.get(\"prediction\")\n",
    "    if not isinstance(pred, str) or not pred.strip():\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    prediction = pred.strip()\n",
    "    original = item[\"original\"].strip()\n",
    "    paraphrases = item.get(\"paraphrases\", [])\n",
    "\n",
    "    # Exclude the first variant\n",
    "    variants = paraphrases[1:]\n",
    "\n",
    "    aligned_preds.append(prediction)\n",
    "    gold_refs.append(original)\n",
    "    variant_refs.append(variants)\n",
    "    aligned_ids.append(f'{item[\"dialogue_id\"]}_{i}')\n",
    "\n",
    "print(f\"\\nâœ… Aligned samples: {len(aligned_preds)}, Skipped: {skipped}\")\n",
    "\n",
    "# æ·»åŠ æ ‡è®°æ¥è¿½è¸ªæœ‰æ•ˆæ€§\n",
    "valid_gold = []\n",
    "valid_variants = []\n",
    "\n",
    "# === ä¿®æ”¹ compute_metrics å‡½æ•° ===\n",
    "def compute_metrics(preds, refs):\n",
    "    valid_preds, valid_refs, valid_idx = [], [], []\n",
    "    bleu_scores_all = [None] * len(preds)\n",
    "    rouge_scores_all = [None] * len(preds)\n",
    "    bert_scores_all = [None] * len(preds)\n",
    "\n",
    "    rouge = load(\"rouge\")\n",
    "    bertscore = load(\"bertscore\")\n",
    "\n",
    "    for idx, (p, r) in enumerate(zip(preds, refs)):\n",
    "        if r:  # refséç©º\n",
    "            valid_preds.append(p)\n",
    "            valid_refs.append(r)\n",
    "            valid_idx.append(idx)\n",
    "\n",
    "    # è®¡ç®—BLEU\n",
    "    bleu_scores = [sacrebleu.sentence_bleu(p, r).score / 100 for p, r in zip(valid_preds, valid_refs)]\n",
    "\n",
    "    # è®¡ç®—ROUGE\n",
    "    rouge_result = rouge.compute(\n",
    "        predictions=valid_preds,\n",
    "        references=valid_refs,\n",
    "        use_stemmer=True,\n",
    "        use_aggregator=False,\n",
    "        rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "    )\n",
    "\n",
    "    # è®¡ç®—BERTScore\n",
    "    F1_max = []\n",
    "    for pred, ref_group in tqdm(zip(valid_preds, valid_refs), total=len(valid_preds)):\n",
    "        best_F1 = max(bertscore.compute(\n",
    "            predictions=[pred] * len(ref_group),\n",
    "            references=ref_group,\n",
    "            lang=\"en\",\n",
    "            rescale_with_baseline=False,\n",
    "            use_fast_tokenizer=True\n",
    "        )[\"f1\"])\n",
    "        F1_max.append(best_F1)\n",
    "\n",
    "    # æŠŠè®¡ç®—å¥½çš„æœ‰æ•ˆåˆ†æ•°æ”¾å›åŸå§‹ä½ç½®ï¼Œæ— æ•ˆä½ç½®å¡«å……None\n",
    "    for i, idx in enumerate(valid_idx):\n",
    "        bleu_scores_all[idx] = bleu_scores[i]\n",
    "        rouge_scores_all[idx] = rouge_result[\"rougeL\"][i]\n",
    "        bert_scores_all[idx] = F1_max[i]\n",
    "\n",
    "    metrics = {\n",
    "        \"BLEU\": sum(bleu_scores) / len(bleu_scores),\n",
    "        \"ROUGE-L\": sum(rouge_result['rougeL']) / len(rouge_result['rougeL']),\n",
    "        \"BERTScore_F1\": sum(F1_max) / len(F1_max)\n",
    "    }\n",
    "\n",
    "    return metrics, bleu_scores_all, rouge_scores_all, bert_scores_all\n",
    "\n",
    "# === Metrics: Prediction vs Gold ===\n",
    "gold_metrics, gold_bleu, gold_rouge, gold_f1 = compute_metrics(aligned_preds, [[ref] for ref in gold_refs])\n",
    "\n",
    "# === Metrics: Prediction vs Variants ===\n",
    "variant_metrics, variant_bleu, variant_rouge, variant_f1 = compute_metrics(aligned_preds, variant_refs)\n",
    "\n",
    "# === æ‰“å°ç»“æœ ===\n",
    "print(\"\\nğŸ“Š Evaluation (Prediction vs Gold):\")\n",
    "print(f\"BLEU (avg)      : {gold_metrics['BLEU']:.4f}\")\n",
    "print(f\"ROUGE-L (avg)   : {gold_metrics['ROUGE-L']:.4f}\")\n",
    "print(f\"BERTScore F1    : {gold_metrics['BERTScore_F1']:.4f}\")\n",
    "\n",
    "print(\"\\nğŸ“Š Evaluation (Prediction vs Variants, excluding first):\")\n",
    "print(f\"BLEU (avg)      : {variant_metrics['BLEU']:.4f}\")\n",
    "print(f\"ROUGE-L (avg)   : {variant_metrics['ROUGE-L']:.4f}\")\n",
    "print(f\"BERTScore F1    : {variant_metrics['BERTScore_F1']:.4f}\")\n",
    "\n",
    "# === ä¿å­˜ CSVï¼ˆç¡®ä¿é•¿åº¦ä¸€è‡´ï¼‰ ===\n",
    "df = pd.DataFrame({\n",
    "    \"sample_id\": aligned_ids,\n",
    "    \"prediction\": aligned_preds,\n",
    "    \"gold_reference\": gold_refs,\n",
    "    \"variant_references\": [\" â€¢ \" + \"\\n â€¢ \".join(v) for v in variant_refs],\n",
    "    \"BLEU_gold\": gold_bleu,\n",
    "    \"ROUGE-L_gold\": gold_rouge,\n",
    "    \"BERTScore_F1_gold\": gold_f1,\n",
    "    \"BLEU_variants\": variant_bleu,\n",
    "    \"ROUGE-L_variants\": variant_rouge,\n",
    "    \"BERTScore_F1_variants\": variant_f1,\n",
    "})\n",
    "\n",
    "# ç©ºç¼ºä½ç½®å¯èƒ½ä¸ºNoneï¼Œå¯ä»¥å¡«å……ä¸ºNaNæˆ–å…¶ä»–å€¼\n",
    "df.to_csv(\"prediction/fine_zero_automatic_scores_separated.csv\", index=False)\n",
    "print(\"ğŸ“ Saved to 'fine_zero_automatic_scores_separated.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef24ac51-18b6-4218-becb-cb506557d258",
   "metadata": {},
   "source": [
    "fine-tuned qwen3â€”â€”few shotï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d28468c5-d111-43d5-88e8-874af2be8843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Aligned samples: 716, Skipped: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/716 [00:00<?, ?it/s]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 716/716 [01:03<00:00, 11.25it/s]\n",
      "  0%|                                                   | 0/715 [00:00<?, ?it/s]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 715/715 [02:42<00:00,  4.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Evaluation (Prediction vs Gold):\n",
      "BLEU (avg)      : 0.0628\n",
      "ROUGE-L (avg)   : 0.2124\n",
      "BERTScore F1    : 0.8878\n",
      "\n",
      "ğŸ“Š Evaluation (Prediction vs Variants, excluding first):\n",
      "BLEU (avg)      : 0.0931\n",
      "ROUGE-L (avg)   : 0.2724\n",
      "BERTScore F1    : 0.8865\n",
      "ğŸ“ Saved to 'fine_zero_automatic_scores_separated.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from evaluate import load\n",
    "from tqdm import tqdm\n",
    "import sacrebleu\n",
    "\n",
    "# === æ–‡ä»¶è·¯å¾„ ===\n",
    "pred_file = \"prediction/finetuned_fewshot_output.json\"\n",
    "para_file = \"doctor_paraphrase_references_with_context.json\"\n",
    "\n",
    "# === åŠ è½½æ•°æ® ===\n",
    "with open(pred_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    predictions = json.load(f)\n",
    "\n",
    "with open(para_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    para_list = json.load(f)\n",
    "\n",
    "# === æ•°æ®å¯¹é½ ===\n",
    "aligned_preds = []\n",
    "gold_refs = []\n",
    "variant_refs = []\n",
    "aligned_ids = []\n",
    "skipped = 0\n",
    "\n",
    "for i, item in enumerate(para_list):\n",
    "    if i >= len(predictions):\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    pred_item = predictions[i]\n",
    "    pred = pred_item.get(\"generated_followup\") or pred_item.get(\"prediction\")\n",
    "    if not isinstance(pred, str) or not pred.strip():\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    prediction = pred.strip()\n",
    "    original = item[\"original\"].strip()\n",
    "    paraphrases = item.get(\"paraphrases\", [])\n",
    "\n",
    "    # Exclude the first variant\n",
    "    variants = paraphrases[1:]\n",
    "\n",
    "    aligned_preds.append(prediction)\n",
    "    gold_refs.append(original)\n",
    "    variant_refs.append(variants)\n",
    "    aligned_ids.append(f'{item[\"dialogue_id\"]}_{i}')\n",
    "\n",
    "print(f\"\\nâœ… Aligned samples: {len(aligned_preds)}, Skipped: {skipped}\")\n",
    "\n",
    "# æ·»åŠ æ ‡è®°æ¥è¿½è¸ªæœ‰æ•ˆæ€§\n",
    "valid_gold = []\n",
    "valid_variants = []\n",
    "\n",
    "# === ä¿®æ”¹ compute_metrics å‡½æ•° ===\n",
    "def compute_metrics(preds, refs):\n",
    "    valid_preds, valid_refs, valid_idx = [], [], []\n",
    "    bleu_scores_all = [None] * len(preds)\n",
    "    rouge_scores_all = [None] * len(preds)\n",
    "    bert_scores_all = [None] * len(preds)\n",
    "\n",
    "    rouge = load(\"rouge\")\n",
    "    bertscore = load(\"bertscore\")\n",
    "\n",
    "    for idx, (p, r) in enumerate(zip(preds, refs)):\n",
    "        if r:  # refséç©º\n",
    "            valid_preds.append(p)\n",
    "            valid_refs.append(r)\n",
    "            valid_idx.append(idx)\n",
    "\n",
    "    # è®¡ç®—BLEU\n",
    "    bleu_scores = [sacrebleu.sentence_bleu(p, r).score / 100 for p, r in zip(valid_preds, valid_refs)]\n",
    "\n",
    "    # è®¡ç®—ROUGE\n",
    "    rouge_result = rouge.compute(\n",
    "        predictions=valid_preds,\n",
    "        references=valid_refs,\n",
    "        use_stemmer=True,\n",
    "        use_aggregator=False,\n",
    "        rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "    )\n",
    "\n",
    "    # è®¡ç®—BERTScore\n",
    "    F1_max = []\n",
    "    for pred, ref_group in tqdm(zip(valid_preds, valid_refs), total=len(valid_preds)):\n",
    "        best_F1 = max(bertscore.compute(\n",
    "            predictions=[pred] * len(ref_group),\n",
    "            references=ref_group,\n",
    "            lang=\"en\",\n",
    "            rescale_with_baseline=False,\n",
    "            use_fast_tokenizer=True\n",
    "        )[\"f1\"])\n",
    "        F1_max.append(best_F1)\n",
    "\n",
    "    # æŠŠè®¡ç®—å¥½çš„æœ‰æ•ˆåˆ†æ•°æ”¾å›åŸå§‹ä½ç½®ï¼Œæ— æ•ˆä½ç½®å¡«å……None\n",
    "    for i, idx in enumerate(valid_idx):\n",
    "        bleu_scores_all[idx] = bleu_scores[i]\n",
    "        rouge_scores_all[idx] = rouge_result[\"rougeL\"][i]\n",
    "        bert_scores_all[idx] = F1_max[i]\n",
    "\n",
    "    metrics = {\n",
    "        \"BLEU\": sum(bleu_scores) / len(bleu_scores),\n",
    "        \"ROUGE-L\": sum(rouge_result['rougeL']) / len(rouge_result['rougeL']),\n",
    "        \"BERTScore_F1\": sum(F1_max) / len(F1_max)\n",
    "    }\n",
    "\n",
    "    return metrics, bleu_scores_all, rouge_scores_all, bert_scores_all\n",
    "\n",
    "# === Metrics: Prediction vs Gold ===\n",
    "gold_metrics, gold_bleu, gold_rouge, gold_f1 = compute_metrics(aligned_preds, [[ref] for ref in gold_refs])\n",
    "\n",
    "# === Metrics: Prediction vs Variants ===\n",
    "variant_metrics, variant_bleu, variant_rouge, variant_f1 = compute_metrics(aligned_preds, variant_refs)\n",
    "\n",
    "# === æ‰“å°ç»“æœ ===\n",
    "print(\"\\nğŸ“Š Evaluation (Prediction vs Gold):\")\n",
    "print(f\"BLEU (avg)      : {gold_metrics['BLEU']:.4f}\")\n",
    "print(f\"ROUGE-L (avg)   : {gold_metrics['ROUGE-L']:.4f}\")\n",
    "print(f\"BERTScore F1    : {gold_metrics['BERTScore_F1']:.4f}\")\n",
    "\n",
    "print(\"\\nğŸ“Š Evaluation (Prediction vs Variants, excluding first):\")\n",
    "print(f\"BLEU (avg)      : {variant_metrics['BLEU']:.4f}\")\n",
    "print(f\"ROUGE-L (avg)   : {variant_metrics['ROUGE-L']:.4f}\")\n",
    "print(f\"BERTScore F1    : {variant_metrics['BERTScore_F1']:.4f}\")\n",
    "\n",
    "# === ä¿å­˜ CSVï¼ˆç¡®ä¿é•¿åº¦ä¸€è‡´ï¼‰ ===\n",
    "df = pd.DataFrame({\n",
    "    \"sample_id\": aligned_ids,\n",
    "    \"prediction\": aligned_preds,\n",
    "    \"gold_reference\": gold_refs,\n",
    "    \"variant_references\": [\" â€¢ \" + \"\\n â€¢ \".join(v) for v in variant_refs],\n",
    "    \"BLEU_gold\": gold_bleu,\n",
    "    \"ROUGE-L_gold\": gold_rouge,\n",
    "    \"BERTScore_F1_gold\": gold_f1,\n",
    "    \"BLEU_variants\": variant_bleu,\n",
    "    \"ROUGE-L_variants\": variant_rouge,\n",
    "    \"BERTScore_F1_variants\": variant_f1,\n",
    "})\n",
    "\n",
    "# ç©ºç¼ºä½ç½®å¯èƒ½ä¸ºNoneï¼Œå¯ä»¥å¡«å……ä¸ºNaNæˆ–å…¶ä»–å€¼\n",
    "df.to_csv(\"prediction/fine_few_automatic_scores_separated.csv\", index=False)\n",
    "print(\"ğŸ“ Saved to 'fine_few_automatic_scores_separated.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a3eb96f7-8745-4dfd-9563-0cae426498ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Aligned samples: 729, Skipped: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/729 [00:00<?, ?it/s]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 729/729 [02:34<00:00,  4.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Evaluation (Gold vs Variants):\n",
      "BLEU (avg)      : 0.2494\n",
      "ROUGE-L (avg)   : 0.2776\n",
      "BERTScore F1    : 0.9191\n",
      "ğŸ“ Saved to 'prediction/gold_vs_variants_similarity_scores.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from evaluate import load\n",
    "from tqdm import tqdm\n",
    "import sacrebleu\n",
    "\n",
    "# === æ–‡ä»¶è·¯å¾„ ===\n",
    "para_file = \"doctor_paraphrase_references_with_context.json\"\n",
    "\n",
    "# === åŠ è½½å‚è€ƒæ•°æ® ===\n",
    "with open(para_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    para_list = json.load(f)\n",
    "\n",
    "# === æ•°æ®æ•´ç†ï¼šgold vs 9ä¸ªvariantsï¼ˆæ’é™¤ç¬¬ä¸€ä¸ªï¼‰===\n",
    "gold_refs = []\n",
    "variant_refs = []\n",
    "aligned_ids = []\n",
    "skipped = 0\n",
    "\n",
    "for i, item in enumerate(para_list):\n",
    "    original = item.get(\"original\", \"\").strip()\n",
    "    paraphrases = item.get(\"paraphrases\", [])\n",
    "\n",
    "    if not original or len(paraphrases) < 2:\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    gold_refs.append(original)\n",
    "    # æ’é™¤ç¬¬ä¸€ä¸ªparaphraseï¼ˆå‡è®¾æ˜¯è‡ªåŠ¨ç”Ÿæˆæœ€æ¥è¿‘goldçš„ï¼‰\n",
    "    variant_refs.append(paraphrases[1:])\n",
    "    aligned_ids.append(f'{item[\"dialogue_id\"]}_{i}')\n",
    "\n",
    "print(f\"\\nâœ… Aligned samples: {len(gold_refs)}, Skipped: {skipped}\")\n",
    "\n",
    "# === è®¡ç®—è¯„ä¼°æŒ‡æ ‡ ===\n",
    "def compute_similarity(gold_refs, variant_groups):\n",
    "    bleu_scores = []\n",
    "    rouge = load(\"rouge\")\n",
    "    bertscore = load(\"bertscore\")\n",
    "\n",
    "    rouge_gold = []\n",
    "    bert_f1_max = []\n",
    "\n",
    "    for gold, variants in tqdm(zip(gold_refs, variant_groups), total=len(gold_refs)):\n",
    "        # --- BLEU ---\n",
    "        bleu = sacrebleu.sentence_bleu(gold, variants).score / 100\n",
    "        bleu_scores.append(bleu)\n",
    "\n",
    "        # --- ROUGE ---\n",
    "        rouge_result = rouge.compute(\n",
    "            predictions=[gold] * len(variants),\n",
    "            references=variants,\n",
    "            use_stemmer=True,\n",
    "            use_aggregator=False,\n",
    "            rouge_types=[\"rougeL\"]\n",
    "        )\n",
    "        rouge_avg = sum(rouge_result[\"rougeL\"]) / len(rouge_result[\"rougeL\"])\n",
    "        rouge_gold.append(rouge_avg)\n",
    "\n",
    "        # --- BERTScore (å– F1 æœ€å¤§å€¼) ---\n",
    "        bert_result = bertscore.compute(\n",
    "            predictions=[gold] * len(variants),\n",
    "            references=variants,\n",
    "            lang=\"en\",\n",
    "            rescale_with_baseline=False,\n",
    "            use_fast_tokenizer=True\n",
    "        )\n",
    "        best_f1 = max(bert_result[\"f1\"])\n",
    "        bert_f1_max.append(best_f1)\n",
    "\n",
    "    return bleu_scores, rouge_gold, bert_f1_max\n",
    "\n",
    "# === æ‰§è¡Œè¯„ä¼° ===\n",
    "bleu_scores, rouge_scores, bert_f1_scores = compute_similarity(gold_refs, variant_refs)\n",
    "\n",
    "# === è¾“å‡ºå¹³å‡ç»“æœ ===\n",
    "print(\"\\nğŸ“Š Evaluation (Gold vs Variants):\")\n",
    "print(f\"BLEU (avg)      : {sum(bleu_scores) / len(bleu_scores):.4f}\")\n",
    "print(f\"ROUGE-L (avg)   : {sum(rouge_scores) / len(rouge_scores):.4f}\")\n",
    "print(f\"BERTScore F1    : {sum(bert_f1_scores) / len(bert_f1_scores):.4f}\")\n",
    "\n",
    "# === ä¿å­˜ç»“æœ ===\n",
    "df = pd.DataFrame({\n",
    "    \"sample_id\": aligned_ids,\n",
    "    \"gold_reference\": gold_refs,\n",
    "    \"variant_references\": [\" â€¢ \" + \"\\n â€¢ \".join(v) for v in variant_refs],\n",
    "    \"BLEU_gold_vs_variants\": bleu_scores,\n",
    "    \"ROUGE-L_gold_vs_variants\": rouge_scores,\n",
    "    \"BERTScore_F1_gold_vs_variants\": bert_f1_scores\n",
    "})\n",
    "\n",
    "df.to_csv(\"prediction/gold_vs_variants_similarity_scores.csv\", index=False)\n",
    "print(\"ğŸ“ Saved to 'prediction/gold_vs_variants_similarity_scores.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68178ae6-6603-4cc0-8328-7c5e3e38340b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
